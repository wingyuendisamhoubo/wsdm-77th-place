{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cff47da",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 19.39064,
     "end_time": "2024-07-11T07:22:59.761341",
     "exception": false,
     "start_time": "2024-07-11T07:22:40.370701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    Gemma2ForSequenceClassification,\n",
    "    GemmaTokenizerFast,\n",
    "    Gemma2Config,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "861ae1d5-8686-427b-be28-1a1c787fe764",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "aug = False\n",
    "combined = False\n",
    "use_former = False\n",
    "QLoRA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2e44b0",
   "metadata": {
    "papermill": {
     "duration": 0.010104,
     "end_time": "2024-07-11T07:22:59.781961",
     "exception": false,
     "start_time": "2024-07-11T07:22:59.771857",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e640b1a3",
   "metadata": {
    "papermill": {
     "duration": 0.020577,
     "end_time": "2024-07-11T07:22:59.812606",
     "exception": false,
     "start_time": "2024-07-11T07:22:59.792029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    output_dir: str = \"output29\"\n",
    "    checkpoint: str = \"gemma2-9b-postpretrained-lmsys\" #\"autodl-tmp/gemma-2-9b-it-bnb-4bit\"   \"unsloth/gemma-2-9b-it-bnb-4bit\"  # 4-bit quantized gemma-2-9b-instruct\n",
    "    lora_dir: str = \"output4/checkpoint-4844\"\n",
    "    max_length: int = 3072\n",
    "    n_splits: int = 100\n",
    "    fold_idx: int = 0\n",
    "    optim_type: str = \"adamw_8bit\"\n",
    "    per_device_train_batch_size: int = 8\n",
    "    #gradient_accumulation_steps: int = 4  # global batch size is 8 \n",
    "    per_device_eval_batch_size: int = 8\n",
    "    n_epochs: int = 1\n",
    "    freeze_layers: int = 0  # there're 42 layers in total, we don't add adapters to the first 16 layers\n",
    "    lr: float = 1e-5\n",
    "    warmup_steps: int = 100\n",
    "    lora_r: int = 64\n",
    "    lora_alpha: float = 128\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_bias: str = \"none\"\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb567bf",
   "metadata": {
    "papermill": {
     "duration": 0.009879,
     "end_time": "2024-07-11T07:22:59.832748",
     "exception": false,
     "start_time": "2024-07-11T07:22:59.822869",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b96d53b",
   "metadata": {
    "papermill": {
     "duration": 0.074241,
     "end_time": "2024-07-11T07:22:59.918039",
     "exception": false,
     "start_time": "2024-07-11T07:22:59.843798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output31\",\n",
    "    overwrite_output_dir=True,\n",
    "    report_to=\"wandb\",\n",
    "    num_train_epochs=config.n_epochs,\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    #gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "    logging_strategy=\"steps\", \n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=2000,\n",
    "    optim=config.optim_type,\n",
    "    fp16=True,\n",
    "    learning_rate=config.lr,\n",
    "    warmup_steps=config.warmup_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883b0309",
   "metadata": {
    "papermill": {
     "duration": 0.009852,
     "end_time": "2024-07-11T07:22:59.938160",
     "exception": false,
     "start_time": "2024-07-11T07:22:59.928308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### LoRA config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39685999",
   "metadata": {
    "papermill": {
     "duration": 0.017924,
     "end_time": "2024-07-11T07:22:59.966078",
     "exception": false,
     "start_time": "2024-07-11T07:22:59.948154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    # only target self-attention\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"down_proj\",\"up_proj\",\"o_proj\",\"gate_proj\"],\n",
    "    layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=config.lora_bias,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    modules_to_save=['score']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab4797d",
   "metadata": {
    "papermill": {
     "duration": 0.009988,
     "end_time": "2024-07-11T07:22:59.985967",
     "exception": false,
     "start_time": "2024-07-11T07:22:59.975979",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Instantiate the tokenizer & model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "758b349f",
   "metadata": {
    "papermill": {
     "duration": 4.152643,
     "end_time": "2024-07-11T07:23:04.148816",
     "exception": false,
     "start_time": "2024-07-11T07:22:59.996173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = GemmaTokenizerFast.from_pretrained(config.checkpoint, local_files_only=True)\n",
    "tokenizer.add_eos_token = True  # We'll add <eos> at the end\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d38c5b98-7079-4620-8300-34688c70fc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "QLoRA = True\n",
    "if QLoRA:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit = True,\n",
    "            bnb_4bit_quant_type = \"nf4\", #nf4 or fp4\n",
    "            bnb_4bit_use_double_quant = False,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            llm_int8_skip_modules = [\"score\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96b17beb-5dab-474a-8154-0596c4fc670e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f2872621154f6b892fbb35ef14650f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    config.checkpoint,\n",
    "    num_labels=2,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config = bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae308faf-de09-4463-a6bb-23f25c5c4793",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.num_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cb6196f-a798-4d5e-a5a5-61149332af30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=3584, out_features=2, bias=False)\n"
     ]
    }
   ],
   "source": [
    "print(model.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7fee40a-fa8e-4804-9e92-8336660a9e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fb63ddf-8923-45f5-932d-0d250196b23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score = torch.nn.Linear(in_features=3584, out_features=2, bias=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "202e87ed-b04a-4b2c-876b-4bf2ead8d6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=3584, out_features=2, bias=False)\n"
     ]
    }
   ],
   "source": [
    "print(model.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88b2d3e1-1ccc-4ae0-9808-4558c18c6ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma2ForSequenceClassification(\n",
       "      (model): Gemma2Model(\n",
       "        (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-41): 42 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=3584, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3584, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): Gemma2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=3584, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3584, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=3584, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=3584, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1537baf4-a050-4851-abf0-ff71e40d7c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.score.modules_to_save.default.weight\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if '.modules_to_save.' in name:\n",
    "        print(name)\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78d73e57",
   "metadata": {
    "papermill": {
     "duration": 0.024546,
     "end_time": "2024-07-11T07:26:42.508563",
     "exception": false,
     "start_time": "2024-07-11T07:26:42.484017",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 216,079,360 || all params: 9,457,792,512 || trainable%: 2.2847\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96d84d90-2e61-4003-a2fd-47d95a51ccae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen parameter: base_model.model.model.embed_tokens.weight\n",
      "Frozen parameter: base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.0.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.0.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.0.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.0.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.0.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.0.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.1.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.1.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.1.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.1.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.1.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.1.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.2.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.2.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.2.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.2.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.2.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.2.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.3.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.3.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.3.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.3.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.3.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.3.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.4.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.4.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.4.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.4.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.4.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.4.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.5.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.5.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.5.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.5.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.5.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.5.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.6.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.6.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.6.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.6.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.6.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.6.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.7.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.7.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.7.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.7.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.7.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.7.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.8.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.8.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.8.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.8.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.8.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.8.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.9.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.9.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.9.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.9.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.9.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.9.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.10.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.10.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.10.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.10.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.10.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.10.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.11.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.11.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.11.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.11.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.11.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.11.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.12.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.12.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.12.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.12.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.12.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.12.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.13.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.13.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.13.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.13.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.13.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.13.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.14.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.14.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.14.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.14.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.14.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.14.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.15.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.15.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.15.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.15.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.15.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.15.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.16.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.16.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.16.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.16.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.16.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.16.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.17.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.17.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.17.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.17.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.17.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.17.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.18.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.18.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.18.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.18.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.18.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.18.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.19.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.19.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.19.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.19.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.19.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.19.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.20.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.20.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.20.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.20.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.20.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.20.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.21.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.21.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.21.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.21.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.21.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.21.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.22.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.22.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.22.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.22.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.22.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.22.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.23.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.23.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.23.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.23.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.23.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.23.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.24.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.24.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.24.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.24.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.24.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.24.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.25.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.25.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.25.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.25.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.25.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.25.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.26.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.26.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.26.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.26.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.26.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.26.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.27.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.27.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.27.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.27.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.27.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.27.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.28.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.28.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.28.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.28.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.28.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.28.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.29.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.29.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.29.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.29.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.29.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.29.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.30.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.30.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.30.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.30.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.30.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.30.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.31.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.31.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.31.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.31.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.31.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.31.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.32.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.32.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.32.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.32.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.32.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.32.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.32.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.32.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.32.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.32.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.32.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.32.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.32.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.32.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.32.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.32.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.32.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.32.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.32.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.32.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.32.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.33.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.33.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.33.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.33.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.33.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.33.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.33.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.33.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.33.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.33.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.33.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.33.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.33.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.33.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.33.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.33.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.33.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.33.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.33.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.33.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.33.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.33.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.33.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.33.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.33.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.34.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.34.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.34.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.34.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.34.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.34.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.34.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.34.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.34.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.34.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.34.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.34.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.34.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.34.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.34.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.34.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.34.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.34.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.34.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.34.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.34.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.35.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.35.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.35.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.35.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.35.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.35.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.35.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.35.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.35.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.35.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.35.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.35.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.35.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.35.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.35.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.35.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.35.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.35.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.35.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.35.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.35.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.36.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.36.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.36.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.36.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.36.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.36.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.36.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.36.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.36.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.36.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.36.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.36.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.36.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.36.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.36.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.36.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.36.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.36.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.36.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.36.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.36.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.36.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.36.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.36.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.36.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.37.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.37.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.37.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.37.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.37.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.37.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.37.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.37.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.37.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.37.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.37.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.37.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.37.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.37.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.37.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.37.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.37.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.37.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.37.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.37.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.37.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.37.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.37.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.37.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.37.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.38.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.38.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.38.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.38.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.38.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.38.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.38.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.38.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.38.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.38.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.38.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.38.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.38.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.38.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.38.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.38.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.38.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.38.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.38.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.38.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.38.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.38.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.38.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.38.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.38.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.39.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.39.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.39.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.39.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.39.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.39.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.39.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.39.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.39.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.39.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.39.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.39.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.39.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.39.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.39.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.39.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.39.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.39.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.39.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.39.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.39.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.39.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.39.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.39.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.39.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.40.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.40.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.40.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.40.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.40.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.40.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.40.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.40.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.40.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.40.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.40.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.40.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.40.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.40.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.40.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.40.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.40.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.40.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.40.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.40.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.40.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.40.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.40.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.40.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.40.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.41.self_attn.q_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.41.self_attn.q_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.41.self_attn.q_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.41.self_attn.k_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.41.self_attn.k_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.41.self_attn.k_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.41.self_attn.v_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.41.self_attn.v_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.41.self_attn.v_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.41.self_attn.o_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.41.self_attn.o_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.41.self_attn.o_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.41.mlp.gate_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.41.mlp.gate_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.41.mlp.gate_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.41.mlp.up_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.41.mlp.up_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.41.mlp.up_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.41.mlp.down_proj.base_layer.weight\n",
      "Training parameter: base_model.model.model.layers.41.mlp.down_proj.lora_A.default.weight\n",
      "Training parameter: base_model.model.model.layers.41.mlp.down_proj.lora_B.default.weight\n",
      "Frozen parameter: base_model.model.model.layers.41.input_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.41.post_attention_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.41.pre_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.layers.41.post_feedforward_layernorm.weight\n",
      "Frozen parameter: base_model.model.model.norm.weight\n",
      "Frozen parameter: base_model.model.score.original_module.weight\n",
      "Training parameter: base_model.model.score.modules_to_save.default.weight\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Training parameter: {name}\")\n",
    "    else:\n",
    "        print(f\"Frozen parameter: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a360fb0-0c32-4e34-bc33-e3a414519876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModulesToSaveWrapper(\n",
      "  (original_module): Linear(in_features=3584, out_features=2, bias=False)\n",
      "  (modules_to_save): ModuleDict(\n",
      "    (default): Linear(in_features=3584, out_features=2, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1cece4d-4752-4b77-bc6d-f016667b9ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competition data has shape (48439, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00007cff95d7f7974642a785aca248b0f26e60d3312fac...</td>\n",
       "      <td>vie po Slovensky?</td>\n",
       "      <td>no, hovorm po slovensky. Ako vm mem pomc?</td>\n",
       "      <td>no, ve som tu! Mem ti pomc s otzkami al...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>o1-preview</td>\n",
       "      <td>reka-core-20240904</td>\n",
       "      <td>Slovak</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id              prompt  \\\n",
       "0  00007cff95d7f7974642a785aca248b0f26e60d3312fac...  vie po Slovensky?   \n",
       "\n",
       "                                         response_a  \\\n",
       "0  no, hovorm po slovensky. Ako vm mem pomc?   \n",
       "\n",
       "                                          response_b   winner     model_a  \\\n",
       "0  no, ve som tu! Mem ti pomc s otzkami al...  model_a  o1-preview   \n",
       "\n",
       "              model_b language  \n",
       "0  reka-core-20240904   Slovak  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"wsdm-cup-multilingual-chatbot-arena/train.parquet\") \n",
    "#df_add = pd.read_parquet(\"lmsys_39k.parquet\") \n",
    "#df = pd.concat([df, df_add]).reset_index(drop=True)\n",
    "#df = df.drop_duplicates(subset=['prompt', 'response_a', 'response_b',], keep='last').reset_index(drop=True)\n",
    "df[\"id\"] = df[\"id\"].astype(\"str\")\n",
    "print('Competition data has shape', df.shape )\n",
    "LN = len(df)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0409f8e-e8e0-4783-b913-16703a72ff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_former:\n",
    "    former_df = pd.read_parquet(\"wsdm-cup-multilingual-chatbot-arena/lmsys_39k.parquet\") \n",
    "    former_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e5ddba5-d261-4283-aef7-e255314b0035",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_former:\n",
    "    filtered_former_df = former_df[former_df['turn'] <= 1]\n",
    "    df = pd.concat([df, filtered_former_df], axis=0).reset_index(drop=True)\n",
    "    df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf2bd2b5-dabc-4185-9e4c-896f757ca225",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_former:\n",
    "    df = df.drop(columns=['turn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81131dd7-6dd2-4d89-9d2b-928f7200f4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 60 unique models:\n",
      "{'athene-70b-0725': 0, 'c4ai-aya-expanse-32b': 1, 'chatgpt-4o-latest-20240808': 2, 'chatgpt-4o-latest-20240903': 3, 'claude-3-5-sonnet-20240620': 4, 'claude-3-5-sonnet-20241022': 5, 'claude-3-haiku-20240307': 6, 'claude-3-opus-20240229': 7, 'command-r-08-2024': 8, 'command-r-plus-08-2024': 9, 'deepseek-coder-v2-0724': 10, 'deepseek-v2-api-0628': 11, 'deepseek-v2.5': 12, 'gemini-1.5-flash-001': 13, 'gemini-1.5-flash-002': 14, 'gemini-1.5-flash-8b-001': 15, 'gemini-1.5-flash-8b-exp-0827': 16, 'gemini-1.5-flash-exp-0827': 17, 'gemini-1.5-pro-001': 18, 'gemini-1.5-pro-002': 19, 'gemini-1.5-pro-exp-0827': 20, 'gemma-2-27b-it': 21, 'gemma-2-2b-it': 22, 'gemma-2-9b-it': 23, 'gemma-2-9b-it-simpo': 24, 'glm-4-plus': 25, 'gpt-4-0125-preview': 26, 'gpt-4-1106-preview': 27, 'gpt-4-turbo-2024-04-09': 28, 'gpt-4o-2024-05-13': 29, 'gpt-4o-2024-08-06': 30, 'gpt-4o-mini-2024-07-18': 31, 'grok-2-2024-08-13': 32, 'grok-2-mini-2024-08-13': 33, 'internlm2_5-20b-chat': 34, 'jamba-1.5-large': 35, 'jamba-1.5-mini': 36, 'llama-3.1-405b-instruct-bf16': 37, 'llama-3.1-405b-instruct-fp8': 38, 'llama-3.1-70b-instruct': 39, 'llama-3.1-8b-instruct': 40, 'llama-3.1-nemotron-51b-instruct': 41, 'llama-3.1-nemotron-70b-instruct': 42, 'llama-3.2-1b-instruct': 43, 'llama-3.2-3b-instruct': 44, 'mistral-large-2407': 45, 'mixtral-8x22b-instruct-v0.1': 46, 'o1-mini': 47, 'o1-preview': 48, 'phi-3-medium-4k-instruct': 49, 'qwen-max-0919': 50, 'qwen-plus-0828': 51, 'qwen2-72b-instruct': 52, 'qwen2.5-72b-instruct': 53, 'reka-core-20240722': 54, 'reka-core-20240904': 55, 'reka-flash-20240722': 56, 'reka-flash-20240904': 57, 'yi-lightning': 58, 'yi-lightning-lite': 59}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00007cff95d7f7974642a785aca248b0f26e60d3312fac...</td>\n",
       "      <td>vie po Slovensky?</td>\n",
       "      <td>no, hovorm po slovensky. Ako vm mem pomc?</td>\n",
       "      <td>no, ve som tu! Mem ti pomc s otzkami al...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>48</td>\n",
       "      <td>55</td>\n",
       "      <td>Slovak</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id              prompt  \\\n",
       "0  00007cff95d7f7974642a785aca248b0f26e60d3312fac...  vie po Slovensky?   \n",
       "\n",
       "                                         response_a  \\\n",
       "0  no, hovorm po slovensky. Ako vm mem pomc?   \n",
       "\n",
       "                                          response_b   winner  model_a  \\\n",
       "0  no, ve som tu! Mem ti pomc s otzkami al...  model_a       48   \n",
       "\n",
       "   model_b language  \n",
       "0       55   Slovak  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "m1 = df.model_a.unique()\n",
    "m2 = df.model_b.unique()\n",
    "#l = df.language.unique()\n",
    "m = np.union1d(m1,m2)\n",
    "m = sorted(m)\n",
    "#l = sorted(l)\n",
    "print(f\"There are {len(m)} unique models:\")\n",
    "#print(f\"There are {len(l)} unique languages:\")\n",
    "\n",
    "MAP_model = {x:y for x,y in zip(m,range(len(m)))}\n",
    "#MAP_language = {x:y for x,y in zip(l,range(len(l)))}\n",
    "print(MAP_model)\n",
    "#print(MAP_language)\n",
    "\n",
    "df.model_a = df.model_a.map(MAP_model).astype('int32')\n",
    "df.model_b = df.model_b.map(MAP_model).astype('int32')\n",
    "#df.language = df.language.map(MAP_language).astype('int32')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04df15d6-e02a-4404-b548-b45ec3afc806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48439/48439 [00:30<00:00, 1589.38it/s]\n",
      "100%|| 48439/48439 [01:06<00:00, 723.56it/s]\n",
      "100%|| 48439/48439 [01:08<00:00, 706.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for col in ['prompt', 'response_a', 'response_b']:\n",
    "    df[col] = df[col].fillna('')\n",
    "    text_list = []\n",
    "    if col == 'prompt':\n",
    "        max_no = 500\n",
    "        s_no = 250\n",
    "        e_no = -251\n",
    "    else:\n",
    "        max_no = 700\n",
    "        s_no = 350\n",
    "        e_no = -351\n",
    "    for text in tqdm(df[col]):\n",
    "        encoded = tokenizer(text, return_offsets_mapping=True)\n",
    "        if len(encoded['input_ids']) > max_no:\n",
    "            start_idx, end_idx = encoded['offset_mapping'][s_no]\n",
    "            new_text = text[:end_idx]\n",
    "            # print(len(tokenizer(text[:end_idx])['input_ids']))\n",
    "            start_idx, end_idx = encoded['offset_mapping'][e_no]\n",
    "            # print(len(tokenizer(text[start_idx:])['input_ids']))\n",
    "            new_text = new_text + \"\\n(snip)\\n\" + text[start_idx:]\n",
    "            # print(len(tokenizer(new_text)['input_ids']), new_text)\n",
    "            text = new_text\n",
    "        text_list.append(text)\n",
    "    df[col] = text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a393cc",
   "metadata": {
    "papermill": {
     "duration": 0.01131,
     "end_time": "2024-07-11T07:26:42.531641",
     "exception": false,
     "start_time": "2024-07-11T07:26:42.520331",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Instantiate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ed4329b",
   "metadata": {
    "papermill": {
     "duration": 3.336506,
     "end_time": "2024-07-11T07:26:45.879581",
     "exception": false,
     "start_time": "2024-07-11T07:26:42.543075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "ds = Dataset.from_pandas(df)\n",
    "\n",
    "#ds = load_dataset(\"parquet\", data_files=\"wsdm-cup-multilingual-chatbot-arena/train.parquet\", split=\"train\"  # or \"all\")\n",
    "#ds = ds.select(torch.arange(64))  # We only use the first 100 data for demo purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "908f84b9-1513-4fc0-ad9a-5c0375e80b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer: PreTrainedTokenizerBase, \n",
    "        max_length: int\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.task_prompt = (\"Your task is to pick the best response between response A and response B. Answer only with 'A' or 'B'. Think carefully before answering.\\n\\n\")\n",
    "        \n",
    "    def __call__(self, batch: dict) -> dict:\n",
    "        prompt = [self.task_prompt + \"<prompt>: \" + t for t in batch[\"prompt\"]]\n",
    "        response_a = [\"\\n\\n<response_a>: \" + t for t in batch[\"response_a\"]]\n",
    "        response_b = [\"\\n\\n<response_b>: \" + t for t in batch[\"response_b\"]]\n",
    "        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)\n",
    "        labels=[]\n",
    "        for win in batch[\"winner\"]:\n",
    "            if win == \"model_a\":\n",
    "                label = 0\n",
    "            else:\n",
    "                label = 1\n",
    "            labels.append(label)\n",
    "        return {**tokenized, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c05565ad",
   "metadata": {
    "papermill": {
     "duration": 0.591102,
     "end_time": "2024-07-11T07:26:46.518445",
     "exception": false,
     "start_time": "2024-07-11T07:26:45.927343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca947ee75aaf489aa6b523119ce9eb9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48439 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encode = CustomTokenizer(tokenizer, max_length=config.max_length)\n",
    "ds_ord = ds.map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be493b2",
   "metadata": {
    "papermill": {
     "duration": 0.011769,
     "end_time": "2024-07-11T07:26:46.542515",
     "exception": false,
     "start_time": "2024-07-11T07:26:46.530746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Compute metrics\n",
    "\n",
    "We'll compute the log-loss used in LB and accuracy as a auxiliary metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d9aeba2",
   "metadata": {
    "papermill": {
     "duration": 0.01987,
     "end_time": "2024-07-11T07:26:46.574012",
     "exception": false,
     "start_time": "2024-07-11T07:26:46.554142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds: EvalPrediction) -> dict:\n",
    "    preds = eval_preds.predictions\n",
    "    labels = eval_preds.label_ids\n",
    "    probs = torch.from_numpy(preds).float().softmax(-1).numpy()\n",
    "    loss = log_loss(y_true=labels, y_pred=probs)\n",
    "    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))\n",
    "    return {\"acc\": acc, \"log_loss\": loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db1b974",
   "metadata": {
    "papermill": {
     "duration": 0.011453,
     "end_time": "2024-07-11T07:26:46.597153",
     "exception": false,
     "start_time": "2024-07-11T07:26:46.585700",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Split\n",
    "\n",
    "Here, train and eval is splitted according to their `id % 5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5095723f",
   "metadata": {
    "papermill": {
     "duration": 0.019576,
     "end_time": "2024-07-11T07:26:46.628437",
     "exception": false,
     "start_time": "2024-07-11T07:26:46.608861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "folds = [\n",
    "    (\n",
    "        [i for i in range(len(ds)) if i % config.n_splits != fold_idx],\n",
    "        [i for i in range(len(ds)) if i % config.n_splits == fold_idx]\n",
    "    ) \n",
    "    for fold_idx in range(config.n_splits)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96b1cbb5-8b8f-4dcd-b61a-5d252c2d3c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2278/4216333392.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcolau83857272\u001b[0m (\u001b[33mmarcolau83857272-sun-yat-sen-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/autodl-tmp/wandb/run-20250202_165747-5d1hm0o3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marcolau83857272-sun-yat-sen-university/huggingface/runs/5d1hm0o3' target=\"_blank\">output31</a></strong> to <a href='https://wandb.ai/marcolau83857272-sun-yat-sen-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marcolau83857272-sun-yat-sen-university/huggingface' target=\"_blank\">https://wandb.ai/marcolau83857272-sun-yat-sen-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marcolau83857272-sun-yat-sen-university/huggingface/runs/5d1hm0o3' target=\"_blank\">https://wandb.ai/marcolau83857272-sun-yat-sen-university/huggingface/runs/5d1hm0o3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5995' max='5995' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5995/5995 18:59:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "      <th>Log Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.534800</td>\n",
       "      <td>0.584780</td>\n",
       "      <td>0.696907</td>\n",
       "      <td>0.584788</td>\n",
       "      <td>199.151000</td>\n",
       "      <td>2.435000</td>\n",
       "      <td>0.306000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5995, training_loss=0.5820976180171251, metrics={'train_runtime': 68365.2443, 'train_samples_per_second': 0.701, 'train_steps_per_second': 0.088, 'total_flos': 4.204156111237792e+18, 'train_loss': 0.5820976180171251, 'epoch': 1.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx, eval_idx = folds[config.fold_idx]\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=training_args, \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ds_ord.select(train_idx),\n",
    "    eval_dataset=ds_ord.select(eval_idx),\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c571fab2-947b-45d2-aa44-39cbb8c5e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_with_lora(trainer, model, output_dir):\n",
    "    # tokenizer\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    save_dict = model.state_dict()\n",
    "\n",
    "    # LoRA\n",
    "    lora_weights = {k: v for k, v in model.state_dict().items() if \"lora\" in k}\n",
    "    \n",
    "    # score\n",
    "    score_weights = {k: v for k, v in model.state_dict().items() if \"score\" in k}\n",
    "    final_d = {}\n",
    "    for k, v in save_dict.items():\n",
    "        if \"lora\" in k or \"score\" in k:\n",
    "            final_d[k] = v\n",
    "\n",
    "    # LoRAscore\n",
    "    torch.save(final_d, os.path.join(output_dir, \"lora_score_weights.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b922399-1b0a-495e-8e05-a3c30a2598b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model = lambda: save_model_with_lora(trainer, model, trainer.args.output_dir)\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027dfe3-3978-44cf-b3ca-c9e972ba7b25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 594.352474,
   "end_time": "2024-07-11T07:32:02.541544",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-11T07:22:08.189070",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00232802e5c148dd8bff6543418b2db3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "015c4f02aaec4725a74cd516caf62693": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7c8b9289997540528380b014173c7ee7",
        "IPY_MODEL_3edc3bbe122247f1aebaa31392486c0f",
        "IPY_MODEL_fb023c34b1ec48019ea35570803f3e3f"
       ],
       "layout": "IPY_MODEL_372b627a4ac64949884e398877f03a74"
      }
     },
     "035079b2d9034c8cb38a00e5896650a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_630fd63fe8fd418689f5e8611c308e83",
       "placeholder": "",
       "style": "IPY_MODEL_cd7f2ce9479449d3b7e6636691894291",
       "value": "100/100[00:00&lt;00:00,597.93examples/s]"
      }
     },
     "06e96bec9c0941cc8c9fc8a857ce426e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "07bf2eef54764f2c9afc33ff633f042d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "08e32f985be44703b5315d5e548f06a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5f394cfabe494663800afecba4961894",
       "placeholder": "",
       "style": "IPY_MODEL_3844c38cafae4c6a8ad53771897ce010",
       "value": "config.json:100%"
      }
     },
     "0dac9ca3320749578640b537b11373cc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "10a2fa0ded4f4dfb97484f09815f301e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f189ceb083ea4c218301fc133796de55",
       "max": 100,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_56e540cd812b400c9b155d39cee4d997",
       "value": 100
      }
     },
     "1221c86e01d54c899c2d38fc92b041db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "123eaaacc699485db6a753364bd921ff": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "12748516c2de4d6285274733777db4a0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "17bc051178ec4af58b5a56bc685f5784": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1b3c0a9029cc44c4888fdba4a6e26b33": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1bde51e218bd4d30936a41414174c19b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b4f1222d37bc4b97966a02893cadf5cb",
       "placeholder": "",
       "style": "IPY_MODEL_675b8fdb116b417199dab7e381f0f987",
       "value": "40.6k/40.6k[00:00&lt;00:00,237kB/s]"
      }
     },
     "1d1314bc6eae4736a9a9777936e1caef": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1fac30aa9f2f45b4b5bc88f223bcc1ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2070fe3adc0f4116b29d9e1d2dbd1ec1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "23a07268b89344878c0c5fa42d646bb8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2553964ce4de49eaa160a00a6029adda": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "261b4c53c19e4f89ad3cf864018efc7b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2b40d9c06cba463ebfe518fb139a385c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2d4f2c06140c415a8e4495f98e3d5e90": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d96c78f83d524c4ebef8ea273d33c193",
        "IPY_MODEL_cba5ef2b10ee4dc9958f85dc2eb800d0",
        "IPY_MODEL_3b61ea64eac147c891d22fb3fd6fe3e4"
       ],
       "layout": "IPY_MODEL_1b3c0a9029cc44c4888fdba4a6e26b33"
      }
     },
     "2e71a2a04438496da478f4913d655b99": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2eb078d6454b4071bfc26c4acffdbb64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d892ec73709c4e3b8b5f3b2a21f6f1c4",
        "IPY_MODEL_10a2fa0ded4f4dfb97484f09815f301e",
        "IPY_MODEL_035079b2d9034c8cb38a00e5896650a5"
       ],
       "layout": "IPY_MODEL_cfee0feb51a5443babfb44f78ade7787"
      }
     },
     "30851564d3a94254bfc835d0c97fd3c2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "315c8abe7a5e481f835585b9a8126d12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "372b627a4ac64949884e398877f03a74": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "37306d9594614528bb09f98d50ffdc87": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bce35cbd41094f739598690f306e4edf",
       "max": 1381,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9c61e43993a545289de61f33442bd8cc",
       "value": 1381
      }
     },
     "3844c38cafae4c6a8ad53771897ce010": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "39ada2a57cd640d285205a4a81297822": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3b61ea64eac147c891d22fb3fd6fe3e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_17bc051178ec4af58b5a56bc685f5784",
       "placeholder": "",
       "style": "IPY_MODEL_ebc94822ee63461eba2456be2a2d8aa7",
       "value": "4.24M/4.24M[00:00&lt;00:00,6.03MB/s]"
      }
     },
     "3db49498ae3a4bd1badb9e5bf5fdc2b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3edc3bbe122247f1aebaa31392486c0f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ad8d4ca0ea8d48d1bb1e6ac6b90df058",
       "max": 636,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1221c86e01d54c899c2d38fc92b041db",
       "value": 636
      }
     },
     "45b5c16e0be4474a8f7122130d49baac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "473de84d5dac454690b486ef77501722": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "56978994fc474373927bf383972aeb5a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "56a41228fdb7459aad86e577fbed58ce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f1b3530913454124baab74f1f2e99f4e",
        "IPY_MODEL_b29844da45264f1193c8a496227fa94b",
        "IPY_MODEL_1bde51e218bd4d30936a41414174c19b"
       ],
       "layout": "IPY_MODEL_d442717a25974100a18284beff00f823"
      }
     },
     "56e540cd812b400c9b155d39cee4d997": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "59967546171d4a8aabe42281a6942147": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_be3697bb2eed4793830bef621d363e96",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fe5b3aa0a4574c9780e4da7532696599",
       "value": 1
      }
     },
     "59ea53d17dd94b0bbb00059b9e33aec8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c847a86a5d10486ea1d4f0f16ac59248",
       "max": 6130708068,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_738af43e2566452e9839e64aa62b0906",
       "value": 6130708068
      }
     },
     "5f394cfabe494663800afecba4961894": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "630fd63fe8fd418689f5e8611c308e83": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "675b8fdb116b417199dab7e381f0f987": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6d52561924fc4cdabe696ae33d40b35a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_56978994fc474373927bf383972aeb5a",
       "placeholder": "",
       "style": "IPY_MODEL_1fac30aa9f2f45b4b5bc88f223bcc1ec",
       "value": "Computingchecksums:100%"
      }
     },
     "7112810b0a22411383227e7be9c44c94": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "738af43e2566452e9839e64aa62b0906": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7410e37085b44324930321018af70148": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_787d23eb2d8a4d53b997a15a23057e91",
        "IPY_MODEL_ef1c499bd45841c1b395f53fc075c665",
        "IPY_MODEL_954a602063774fc697f26f030a5dfe6a"
       ],
       "layout": "IPY_MODEL_30851564d3a94254bfc835d0c97fd3c2"
      }
     },
     "74b58213dda748eba3640b3b96ffb23c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7dc4d482635c40669d6ffa1f5eb1a847",
        "IPY_MODEL_ec3ddb8359394c868a1d24a9c57765af",
        "IPY_MODEL_91507fcdc3e74c10ae747bc3d7dcd9b6"
       ],
       "layout": "IPY_MODEL_07bf2eef54764f2c9afc33ff633f042d"
      }
     },
     "76ea86b3510c4a1595ee37069bb1da2b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d477006f55a944a29fec6012fc00abf3",
        "IPY_MODEL_59ea53d17dd94b0bbb00059b9e33aec8",
        "IPY_MODEL_c76172ed9e5e45e49b3f0e5b383f2216"
       ],
       "layout": "IPY_MODEL_12748516c2de4d6285274733777db4a0"
      }
     },
     "771f39a79e3e4c74a4f9d113d67210ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "787d23eb2d8a4d53b997a15a23057e91": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2553964ce4de49eaa160a00a6029adda",
       "placeholder": "",
       "style": "IPY_MODEL_2070fe3adc0f4116b29d9e1d2dbd1ec1",
       "value": "Generatingtrainsplit:"
      }
     },
     "7c8b8e3eaf174b8ebc761e19d089ff31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7c8b9289997540528380b014173c7ee7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_473de84d5dac454690b486ef77501722",
       "placeholder": "",
       "style": "IPY_MODEL_2b40d9c06cba463ebfe518fb139a385c",
       "value": "special_tokens_map.json:100%"
      }
     },
     "7dc4d482635c40669d6ffa1f5eb1a847": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1d1314bc6eae4736a9a9777936e1caef",
       "placeholder": "",
       "style": "IPY_MODEL_7c8b8e3eaf174b8ebc761e19d089ff31",
       "value": "tokenizer.json:100%"
      }
     },
     "8000e2f7f4044de4b9e8c1f44e25f442": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "83cd9e97fe7648359faf558e13c6c81b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "84e140a96afa4b9f90ce2175a2e14043": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "85c9a38e613c4545800825ce0f033af3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "91507fcdc3e74c10ae747bc3d7dcd9b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b8325f2011e84e869869326befa4b65d",
       "placeholder": "",
       "style": "IPY_MODEL_771f39a79e3e4c74a4f9d113d67210ff",
       "value": "17.5M/17.5M[00:00&lt;00:00,34.8MB/s]"
      }
     },
     "9227c267ba7a40339a794403934219ce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "954a602063774fc697f26f030a5dfe6a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ab05dcd3fee448108653297694144a25",
       "placeholder": "",
       "style": "IPY_MODEL_06e96bec9c0941cc8c9fc8a857ce426e",
       "value": "57477/0[00:03&lt;00:00,18505.42examples/s]"
      }
     },
     "9c61e43993a545289de61f33442bd8cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a3fac6db6ed547aa9aa6aa71b89d80d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_08e32f985be44703b5315d5e548f06a8",
        "IPY_MODEL_37306d9594614528bb09f98d50ffdc87",
        "IPY_MODEL_c3d6e1bebc89405d8b13bae6f117e605"
       ],
       "layout": "IPY_MODEL_8000e2f7f4044de4b9e8c1f44e25f442"
      }
     },
     "ab05dcd3fee448108653297694144a25": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ab11fb039b06421f9ebace4229121cab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6d52561924fc4cdabe696ae33d40b35a",
        "IPY_MODEL_59967546171d4a8aabe42281a6942147",
        "IPY_MODEL_ca9361e4de004fdb8185c1f345846a45"
       ],
       "layout": "IPY_MODEL_7112810b0a22411383227e7be9c44c94"
      }
     },
     "ad8d4ca0ea8d48d1bb1e6ac6b90df058": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b1394b1da1bc406fb06dfede4ecd5e9d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b29844da45264f1193c8a496227fa94b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0dac9ca3320749578640b537b11373cc",
       "max": 40634,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_cdf49a8890f84651a3da8149ba5a7bfa",
       "value": 40634
      }
     },
     "b38047dcb6b74c0c9cb4956f1e46a316": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b42e19cb6cd148fcb9ce289f2b8f360f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b4f1222d37bc4b97966a02893cadf5cb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b8325f2011e84e869869326befa4b65d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bce35cbd41094f739598690f306e4edf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "be3697bb2eed4793830bef621d363e96": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c3d6e1bebc89405d8b13bae6f117e605": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ed73e482819544919bc6d7901ead939f",
       "placeholder": "",
       "style": "IPY_MODEL_83cd9e97fe7648359faf558e13c6c81b",
       "value": "1.38k/1.38k[00:00&lt;00:00,125kB/s]"
      }
     },
     "c60344a8edab4a8c9ac7357c5cfedf7e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c6f85ef169d0411c9d97df42ecb6b9de": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c76172ed9e5e45e49b3f0e5b383f2216": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d286ed9ba852413eb67370de7f219e5d",
       "placeholder": "",
       "style": "IPY_MODEL_261b4c53c19e4f89ad3cf864018efc7b",
       "value": "6.13G/6.13G[03:31&lt;00:00,28.4MB/s]"
      }
     },
     "c847a86a5d10486ea1d4f0f16ac59248": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ca9361e4de004fdb8185c1f345846a45": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_00232802e5c148dd8bff6543418b2db3",
       "placeholder": "",
       "style": "IPY_MODEL_ff1b356af4394477bf6ffdad3cd852f6",
       "value": "1/1[00:00&lt;00:00,114.36it/s]"
      }
     },
     "cba5ef2b10ee4dc9958f85dc2eb800d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9227c267ba7a40339a794403934219ce",
       "max": 4241003,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b1394b1da1bc406fb06dfede4ecd5e9d",
       "value": 4241003
      }
     },
     "cd7f2ce9479449d3b7e6636691894291": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "cdf49a8890f84651a3da8149ba5a7bfa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "cfee0feb51a5443babfb44f78ade7787": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d286ed9ba852413eb67370de7f219e5d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d442717a25974100a18284beff00f823": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d477006f55a944a29fec6012fc00abf3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_45b5c16e0be4474a8f7122130d49baac",
       "placeholder": "",
       "style": "IPY_MODEL_b42e19cb6cd148fcb9ce289f2b8f360f",
       "value": "model.safetensors:100%"
      }
     },
     "d892ec73709c4e3b8b5f3b2a21f6f1c4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_23a07268b89344878c0c5fa42d646bb8",
       "placeholder": "",
       "style": "IPY_MODEL_3db49498ae3a4bd1badb9e5bf5fdc2b2",
       "value": "Map:100%"
      }
     },
     "d96c78f83d524c4ebef8ea273d33c193": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_123eaaacc699485db6a753364bd921ff",
       "placeholder": "",
       "style": "IPY_MODEL_315c8abe7a5e481f835585b9a8126d12",
       "value": "tokenizer.model:100%"
      }
     },
     "da4b3ed2364747c580aefa288b10c35e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ebc94822ee63461eba2456be2a2d8aa7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ec3ddb8359394c868a1d24a9c57765af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c6f85ef169d0411c9d97df42ecb6b9de",
       "max": 17518525,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2e71a2a04438496da478f4913d655b99",
       "value": 17518525
      }
     },
     "ed73e482819544919bc6d7901ead939f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ef1c499bd45841c1b395f53fc075c665": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_84e140a96afa4b9f90ce2175a2e14043",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_da4b3ed2364747c580aefa288b10c35e",
       "value": 1
      }
     },
     "f189ceb083ea4c218301fc133796de55": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f1b3530913454124baab74f1f2e99f4e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_39ada2a57cd640d285205a4a81297822",
       "placeholder": "",
       "style": "IPY_MODEL_b38047dcb6b74c0c9cb4956f1e46a316",
       "value": "tokenizer_config.json:100%"
      }
     },
     "fb023c34b1ec48019ea35570803f3e3f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_85c9a38e613c4545800825ce0f033af3",
       "placeholder": "",
       "style": "IPY_MODEL_c60344a8edab4a8c9ac7357c5cfedf7e",
       "value": "636/636[00:00&lt;00:00,57.6kB/s]"
      }
     },
     "fe5b3aa0a4574c9780e4da7532696599": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ff1b356af4394477bf6ffdad3cd852f6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
